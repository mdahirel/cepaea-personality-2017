---
title: "Cepaea 2017"
author: 
  - "**Maxime Dahirel** (script author)"
  - Valentin Gaudu
  - Armelle Ansart
date: "13/02/2020"
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
    code_download: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

(NB: A "classical" .R script version is also available in the same folder as this file, for readers who'd rather use that. That version is much less commented, but both run the same analyses)


## **Introduction**


(see manuscript for details)


## **Starters and data wrangling**
First, let's load the packages we'll need:

```{r load-packages}
library(here)
library(arm)
library(matrixStats)
library(tidyverse)
library(rstan)
library(tidybayes)
library(bayesplot)
library(brms)
library(cowplot)
library(QGglmm)
library(patchwork)
rstan_options(auto_write = TRUE)
options(mc.cores = 2)

N_chains <- 4
N_iter <- 20000 ## default number is 2000
N_warmup <- 10000 ## default number is 1000
N_thin <- 1 ## thinning frequency, in case it is needed to keep models at a manageable memory size ( 1: no thinning)
```

Now we load the data files:
```{r load-datafiles}
## Load main dataset

data <- read_csv("./cepaea-2017-dataset.csv")

### to do: add the abundance dataset for the supplementary
```


```{r data-wrangling-censoring}

data<-data %>% 
  mutate(censored_fpt = as.numeric(fpt>=1200),
         censored_bold = as.numeric(boldness >= 1200)
  )
```

```{r data-wrangling-recoding}

data<-data %>% 
  mutate(s_temp = scale(temperature), #centred and scaled
         c_order.temp = order.temp - 2.5, ## center without scaling
         c_order.b <- tab$order.b - 1.5, ## same
         c_landscape <- -0.5 + as.numeric(landscape == "Marais"), ## same
         ##recoding
         is0b=(band_genotype=="0b")-mean(band_genotype=="0b"),
         is3b=(band_genotype=="3b")-mean(band_genotype=="3b"),
         is5b=(band_genotype=="5b")-mean(band_genotype=="5b")
)

mean_temp <- attr(data$s_temp, "scaled:center")
sd_temp <- attr(data$s_temp, "scaled:scale")
```

```{r data-wrangling-subset}
####add dummy values to be able to use the subset approach without restructuring the table and without
### setting NA flags 
### don't worry, dummy values will be ignored during fitting

data <- data %>% 
  mutate(is.valid.bold <- as.numeric(is.na(tab$boldness) == FALSE), ### if it's not NA, it's valid,
         censored_bold2 <- rep(censored_bold[1:720], rep(1,720)),
         boldness2 <- c(boldness[1:720], rep(1200, 720)) 
         ### and we fill the cells that won't be used with dummy values
  )
```


## **Main model**

Now, let's fit our main model (please see description in the methods and supplementary material of the paper).

```{r multivariate-model}
####FORMULAS

bf_explo <- bf(fpt |cens(censored_fpt) ~ ((is3b+is5b) * c_landscape) * s_temp + c_order.temp +
                    (s_temp | p | box) + (s_temp | q | id), sigma ~ 1, family = lognormal(link_sigma="identity"))

bf_boldness <- bf(boldness2 | subset(is.valid.bold) + cens(censored_bold2) ~ (is3b+is5b) * c_landscape + c_order.b +
                    (1 | p | box) + (1 | q | id), sigma ~ 1, family = lognormal(link_sigma="identity")) ##when sigma not modeled link is identity (half prior?)


### PRIOR
prior_multi <- c(
  set_prior("normal(0, 1)", class = "b",resp=c("boldness2","fpt")), ##### Fixed effects; weakly informative prior for response centred and scaled
  set_prior("normal(log(400),0.5)",class=c("Intercept"),resp=c("boldness2","fpt")),
  set_prior("lkj(2)", class = "cor"),
  set_prior("exponential(1)",class = "sd",resp=c("boldness2","fpt")),
  set_prior("exponential(1)",dpar = "sigma",resp=c("boldness2","fpt"),class="Intercept")
)

### fit models ##NB: the sampler may throw initialization errors at the beginning because it evaluates loglik at log(0)
### but then come out fine
###better prior for intercepts?

###FITTING
mod <- brm(mvbf(bf_explo + bf_boldness, rescor = FALSE),
              data = tab, chains=N_chains/2, iter = N_iter/100, warmup = N_warmup/100, thin = N_thin, prior = prior_multi, seed = 42,
              control=list(adapt_delta = 0.95,max_treedepth=15) #,save_ranef=FALSE
)

#careful, >1Gb with iter =20000 warmup = 10000 if saving everything
# use save_ranef=FALSE to masssively reduce size (if you don't need individual random effects, simply variances, for inference)
```


## **Making figures**

(Fig. 1 is a shell photograph, so we start at Fig.2 for data plots)

### Figure 2


### Figure 3


### Figure 4


## **Supplementary Materials**

We also present a model in Supplementary Materials. 
